{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6bfa616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x165b4d9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16630d580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[INFO] 6-month horizon rolling training for COST completed. Overall MAPE: 21.59%\n",
      "[INFO] Detailed results saved to cost_6months_results1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import talib\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "########################\n",
    "# Configuration for 6-month horizon\n",
    "########################\n",
    "ticker = 'COST'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "n_steps = 5\n",
    "window_size = 200\n",
    "step_size = 10\n",
    "initial_epochs = 100\n",
    "update_epochs = 100\n",
    "initial_lr = 0.0005\n",
    "update_lr = 0.0002\n",
    "train_ratio = 0.9\n",
    "prediction_horizon = 126  # 6-month horizon\n",
    "\n",
    "if not os.path.exists(\"rolling_error_analysis_results\"):\n",
    "    os.makedirs(\"rolling_error_analysis_results\")\n",
    "\n",
    "def load_full_data_helper(ticker, start_date, end_date, window_size, prediction_horizon):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    if stock_data.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    # Shift by prediction_horizon days\n",
    "    stock_data['Close_future'] = stock_data['Close'].shift(-prediction_horizon)\n",
    "    stock_data.dropna(subset=['Close_future'], inplace=True)\n",
    "\n",
    "    # Compute features: BB_Upper, BB_Lower, Volatility_30\n",
    "    # Using a 90-day Bollinger Band as user snippet suggests\n",
    "    upper_band, middle_band, lower_band = talib.BBANDS(stock_data['Close'], timeperiod=90, nbdevup=2, nbdevdn=2)\n",
    "    stock_data['BB_Upper'] = upper_band\n",
    "    stock_data['BB_Lower'] = lower_band\n",
    "\n",
    "    stock_data['Returns'] = stock_data['Close'].pct_change()\n",
    "    stock_data['Volatility_30'] = stock_data['Returns'].rolling(90).std()\n",
    "\n",
    "    # Drop rows with NaNs created by indicators\n",
    "    stock_data.dropna(subset=['BB_Upper', 'BB_Lower', 'Volatility_30', 'Close_future'], inplace=True)\n",
    "\n",
    "    # Check if we still have enough data\n",
    "    if len(stock_data) < window_size:\n",
    "        return None, None, None\n",
    "\n",
    "    # Features are now BB_Upper, BB_Lower, Volatility_30\n",
    "    X_feat = stock_data[['BB_Upper', 'BB_Lower', 'Volatility_30']].values\n",
    "    y_raw = stock_data['Close_future'].values\n",
    "    dates = stock_data['Date'].values\n",
    "\n",
    "    return X_feat, y_raw, dates\n",
    "\n",
    "def load_full_data(ticker, start_date, end_date, window_size, prediction_horizon):\n",
    "    X_feat, y_raw, dates = load_full_data_helper(ticker, start_date, end_date, window_size, prediction_horizon)\n",
    "    if X_feat is None:\n",
    "        return None, None, None\n",
    "    return X_feat, y_raw, dates\n",
    "\n",
    "def extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps):\n",
    "    end_idx = start_idx + window_size\n",
    "    if end_idx > len(X_full):\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    X_window = X_full[start_idx:end_idx]\n",
    "    y_window = y_full[start_idx:end_idx]\n",
    "    dates_window = dates_full[start_idx:end_idx]\n",
    "\n",
    "    def lstm_split(dataX, dataY, n_steps):\n",
    "        X, y = [], []\n",
    "        for i in range(len(dataX)-n_steps+1):\n",
    "            X.append(dataX[i:i+n_steps])\n",
    "            y.append(dataY[i+n_steps-1])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X_samples, y_samples = lstm_split(X_window, y_window, n_steps)\n",
    "    sample_dates = dates_window[n_steps-1:]\n",
    "\n",
    "    feat_scaler = RobustScaler()\n",
    "    target_scaler = RobustScaler()\n",
    "    X_flat = X_window.reshape(len(X_window), -1)\n",
    "    feat_scaler.fit(X_flat)\n",
    "    y_window_reshaped = y_window.reshape(-1,1)\n",
    "    target_scaler.fit(y_window_reshaped)\n",
    "    X_scaled = feat_scaler.transform(X_flat).reshape(X_window.shape)\n",
    "    y_scaled = target_scaler.transform(y_window_reshaped).flatten()\n",
    "    X_samples_scaled, y_samples_scaled = lstm_split(X_scaled, y_scaled, n_steps)\n",
    "\n",
    "    return X_samples_scaled, y_samples_scaled, sample_dates, feat_scaler, target_scaler, (X_window, y_window, dates_window)\n",
    "\n",
    "def create_lstm_model(n_units, learning_rate, n_steps, n_features, n_layers=1):\n",
    "    inputs = Input(shape=(n_steps, n_features))\n",
    "    x = LSTM(n_units, activation='relu', return_sequences=(n_layers > 1))(inputs)\n",
    "    if n_layers > 1:\n",
    "        x = LSTM(n_units, activation='relu')(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model  # Note: we compile after loading weights\n",
    "\n",
    "def train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                    initial_training=False, prev_weights=None,\n",
    "                    initial_epochs=100, update_epochs=100,\n",
    "                    initial_lr=0.0005, update_lr=0.0002,\n",
    "                    n_units=16, batch_size=4):\n",
    "    total_samples = len(X_samples)\n",
    "    train_ratio = 0.9\n",
    "    train_count = int(math.floor(total_samples * train_ratio))\n",
    "    if train_count >= total_samples:\n",
    "        train_count = total_samples - 1\n",
    "\n",
    "    X_train, y_train = X_samples[:train_count], y_samples[:train_count]\n",
    "    X_val, y_val = X_samples[train_count:], y_samples[train_count:]\n",
    "\n",
    "    if X_train.size == 0 or X_val.size == 0:\n",
    "        return None, np.nan\n",
    "\n",
    "    n_steps = X_train.shape[1]\n",
    "    n_features = X_train.shape[2]\n",
    "\n",
    "    if initial_training:\n",
    "        epochs = initial_epochs\n",
    "        lr = initial_lr\n",
    "    else:\n",
    "        epochs = update_epochs\n",
    "        lr = update_lr\n",
    "\n",
    "    model = create_lstm_model(n_units, lr, n_steps, n_features, n_layers=1)\n",
    "\n",
    "    # Load weights before compile to avoid optimizer warnings\n",
    "    if prev_weights is not None:\n",
    "        model.load_weights(prev_weights)\n",
    "\n",
    "    # Compile after loading weights\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=lr))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False,\n",
    "              validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "    y_val_pred_scaled = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = target_scaler.inverse_transform(y_val.reshape(-1,1)).flatten()\n",
    "    y_val_pred_unscaled = target_scaler.inverse_transform(y_val_pred_scaled).flatten()\n",
    "    val_mape = mean_absolute_percentage_error(y_val_unscaled, y_val_pred_unscaled) * 100\n",
    "\n",
    "    return model, val_mape\n",
    "\n",
    "def rolling_training_early_stopping(ticker, start_date, end_date,\n",
    "                                    n_steps=5, window_size=200, step_size=10,\n",
    "                                    initial_epochs=100, update_epochs=100,\n",
    "                                    initial_lr=0.0005, update_lr=0.0002,\n",
    "                                    n_units=16, batch_size=4,\n",
    "                                    prediction_horizon=126):\n",
    "\n",
    "    X_full, y_full, dates_full = load_full_data(ticker, start_date, end_date, window_size, prediction_horizon)\n",
    "    if X_full is None:\n",
    "        return None, None\n",
    "\n",
    "    start_idx = 0\n",
    "    window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "    if window_data[0] is None:\n",
    "        return None, None\n",
    "    X_samples, y_samples, sample_dates, feat_scaler, target_scaler, (Xw, yw, dw) = window_data\n",
    "\n",
    "    model, val_mape = train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                                      initial_training=True, prev_weights=None,\n",
    "                                      initial_epochs=initial_epochs, update_epochs=update_epochs,\n",
    "                                      initial_lr=initial_lr, update_lr=update_lr,\n",
    "                                      n_units=n_units, batch_size=batch_size)\n",
    "\n",
    "    if model is None:\n",
    "        return None, None\n",
    "\n",
    "    weights_path = f\"rolling_error_analysis_results/{ticker}_initial_6months.weights.h5\"\n",
    "    model.save_weights(weights_path)\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    pred_dates = []\n",
    "\n",
    "    iteration = 1\n",
    "    while True:\n",
    "        start_idx += step_size\n",
    "        window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "        if window_data[0] is None:\n",
    "            break\n",
    "        X_samples, y_samples, sample_dates, feat_scaler, target_scaler, (Xw, yw, dw) = window_data\n",
    "\n",
    "        model, val_mape = train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                                          initial_training=False, prev_weights=weights_path,\n",
    "                                          initial_epochs=initial_epochs, update_epochs=update_epochs,\n",
    "                                          initial_lr=initial_lr, update_lr=update_lr,\n",
    "                                          n_units=n_units, batch_size=batch_size)\n",
    "\n",
    "        if model is None:\n",
    "            break\n",
    "\n",
    "        weights_path = f\"rolling_error_analysis_results/{ticker}_iteration_{iteration}_6months.weights.h5\"\n",
    "        model.save_weights(weights_path)\n",
    "\n",
    "        X_test_last = X_samples[-1:]\n",
    "        y_test_last = y_samples[-1:]\n",
    "        y_pred_scaled = model.predict(X_test_last, verbose=0)\n",
    "        y_test_unscaled = target_scaler.inverse_transform(y_test_last.reshape(-1,1)).flatten()\n",
    "        y_pred_unscaled = target_scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "        predictions.append(y_pred_unscaled[0])\n",
    "        actuals.append(y_test_unscaled[0])\n",
    "        pred_dates.append(sample_dates[-1])\n",
    "        iteration += 1\n",
    "\n",
    "    if len(predictions) == 0:\n",
    "        return None, None\n",
    "\n",
    "    df_results = pd.DataFrame({\n",
    "        'Date': pred_dates,\n",
    "        'Actual': actuals,\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "    df_results['Absolute_Error'] = np.abs(df_results['Actual'] - df_results['Predicted'])\n",
    "    df_results['APE'] = df_results['Absolute_Error'] / np.abs(df_results['Actual'])\n",
    "    df_results['MAPE'] = df_results['APE'] * 100.0\n",
    "    overall_mape = df_results['MAPE'].mean()\n",
    "\n",
    "    return overall_mape, df_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mape, results_df = rolling_training_early_stopping(\n",
    "        ticker,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        n_steps=n_steps,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size,\n",
    "        initial_epochs=initial_epochs,\n",
    "        update_epochs=update_epochs,\n",
    "        initial_lr=initial_lr,\n",
    "        update_lr=update_lr,\n",
    "        n_units=16,\n",
    "        batch_size=4,\n",
    "        prediction_horizon=126\n",
    "    )\n",
    "\n",
    "    if mape is not None:\n",
    "        print(f\"[INFO] 6-month horizon rolling training for {ticker} completed. Overall MAPE: {mape:.2f}%\")\n",
    "        results_df.to_csv(\"rolling_error_analysis_results/cost_6months_results1.csv\", index=False)\n",
    "        print(\"[INFO] Detailed results saved to cost_6months_results1.csv\")\n",
    "    else:\n",
    "        print(\"[WARN] Not enough data or no predictions made for COST with 6-month horizon.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32afbcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[INFO] 6-month horizon rolling training for COST completed. Overall MAPE: 2.10%\n",
      "[INFO] Detailed results saved to cost_6months_results1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import talib\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "########################\n",
    "# Configuration for 6-month horizon\n",
    "########################\n",
    "ticker = 'COST'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "n_steps = 1\n",
    "window_size = 10\n",
    "step_size = 5\n",
    "initial_epochs = 100\n",
    "update_epochs = 100\n",
    "initial_lr = 0.0005\n",
    "update_lr = 0.0002\n",
    "train_ratio = 0.9\n",
    "prediction_horizon = 126  # 6-month horizon\n",
    "\n",
    "if not os.path.exists(\"rolling_error_analysis_results\"):\n",
    "    os.makedirs(\"rolling_error_analysis_results\")\n",
    "\n",
    "def load_full_data_helper(ticker, start_date, end_date, window_size, prediction_horizon):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    if stock_data.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    # Shift by prediction_horizon days\n",
    "    stock_data['Close_future'] = stock_data['Close'].shift(-prediction_horizon)\n",
    "    stock_data.dropna(subset=['Close_future'], inplace=True)\n",
    "\n",
    "    # Compute features: BB_Upper, BB_Lower, Volatility_30\n",
    "    # Using a 90-day Bollinger Band as user snippet suggests\n",
    "    upper_band, middle_band, lower_band = talib.BBANDS(stock_data['Close'], timeperiod=90, nbdevup=2, nbdevdn=2)\n",
    "    stock_data['BB_Upper'] = upper_band\n",
    "    stock_data['BB_Lower'] = lower_band\n",
    "\n",
    "    stock_data['Returns'] = stock_data['Close'].pct_change()\n",
    "    stock_data['Volatility_30'] = stock_data['Returns'].rolling(90).std()\n",
    "\n",
    "    # Drop rows with NaNs created by indicators\n",
    "    stock_data.dropna(subset=['BB_Upper', 'BB_Lower', 'Volatility_30', 'Close_future'], inplace=True)\n",
    "\n",
    "    # Check if we still have enough data\n",
    "    if len(stock_data) < window_size:\n",
    "        return None, None, None\n",
    "\n",
    "    # Features are now BB_Upper, BB_Lower, Volatility_30\n",
    "    X_feat = stock_data[['BB_Upper', 'BB_Lower', 'Volatility_30']].values\n",
    "    y_raw = stock_data['Close_future'].values\n",
    "    dates = stock_data['Date'].values\n",
    "\n",
    "    return X_feat, y_raw, dates\n",
    "\n",
    "def load_full_data(ticker, start_date, end_date, window_size, prediction_horizon):\n",
    "    X_feat, y_raw, dates = load_full_data_helper(ticker, start_date, end_date, window_size, prediction_horizon)\n",
    "    if X_feat is None:\n",
    "        return None, None, None\n",
    "    return X_feat, y_raw, dates\n",
    "\n",
    "def extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps):\n",
    "    end_idx = start_idx + window_size\n",
    "    if end_idx > len(X_full):\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    X_window = X_full[start_idx:end_idx]\n",
    "    y_window = y_full[start_idx:end_idx]\n",
    "    dates_window = dates_full[start_idx:end_idx]\n",
    "\n",
    "    def lstm_split(dataX, dataY, n_steps):\n",
    "        X, y = [], []\n",
    "        for i in range(len(dataX)-n_steps+1):\n",
    "            X.append(dataX[i:i+n_steps])\n",
    "            y.append(dataY[i+n_steps-1])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X_samples, y_samples = lstm_split(X_window, y_window, n_steps)\n",
    "    sample_dates = dates_window[n_steps-1:]\n",
    "\n",
    "    feat_scaler = RobustScaler()\n",
    "    target_scaler = RobustScaler()\n",
    "    X_flat = X_window.reshape(len(X_window), -1)\n",
    "    feat_scaler.fit(X_flat)\n",
    "    y_window_reshaped = y_window.reshape(-1,1)\n",
    "    target_scaler.fit(y_window_reshaped)\n",
    "    X_scaled = feat_scaler.transform(X_flat).reshape(X_window.shape)\n",
    "    y_scaled = target_scaler.transform(y_window_reshaped).flatten()\n",
    "    X_samples_scaled, y_samples_scaled = lstm_split(X_scaled, y_scaled, n_steps)\n",
    "\n",
    "    return X_samples_scaled, y_samples_scaled, sample_dates, feat_scaler, target_scaler, (X_window, y_window, dates_window)\n",
    "\n",
    "def create_lstm_model(n_units, learning_rate, n_steps, n_features, n_layers=1):\n",
    "    inputs = Input(shape=(n_steps, n_features))\n",
    "    x = LSTM(n_units, activation='relu', return_sequences=(n_layers > 1))(inputs)\n",
    "    if n_layers > 1:\n",
    "        x = LSTM(n_units, activation='relu')(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model  # Note: we compile after loading weights\n",
    "\n",
    "def train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                    initial_training=False, prev_weights=None,\n",
    "                    initial_epochs=100, update_epochs=100,\n",
    "                    initial_lr=0.0005, update_lr=0.0002,\n",
    "                    n_units=16, batch_size=4):\n",
    "    total_samples = len(X_samples)\n",
    "    train_ratio = 0.9\n",
    "    train_count = int(math.floor(total_samples * train_ratio))\n",
    "    if train_count >= total_samples:\n",
    "        train_count = total_samples - 1\n",
    "\n",
    "    X_train, y_train = X_samples[:train_count], y_samples[:train_count]\n",
    "    X_val, y_val = X_samples[train_count:], y_samples[train_count:]\n",
    "\n",
    "    if X_train.size == 0 or X_val.size == 0:\n",
    "        return None, np.nan\n",
    "\n",
    "    n_steps = X_train.shape[1]\n",
    "    n_features = X_train.shape[2]\n",
    "\n",
    "    if initial_training:\n",
    "        epochs = initial_epochs\n",
    "        lr = initial_lr\n",
    "    else:\n",
    "        epochs = update_epochs\n",
    "        lr = update_lr\n",
    "\n",
    "    model = create_lstm_model(n_units, lr, n_steps, n_features, n_layers=1)\n",
    "\n",
    "    # Load weights before compile to avoid optimizer warnings\n",
    "    if prev_weights is not None:\n",
    "        model.load_weights(prev_weights)\n",
    "\n",
    "    # Compile after loading weights\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=lr))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False,\n",
    "              validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "    y_val_pred_scaled = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = target_scaler.inverse_transform(y_val.reshape(-1,1)).flatten()\n",
    "    y_val_pred_unscaled = target_scaler.inverse_transform(y_val_pred_scaled).flatten()\n",
    "    val_mape = mean_absolute_percentage_error(y_val_unscaled, y_val_pred_unscaled) * 100\n",
    "\n",
    "    return model, val_mape\n",
    "\n",
    "def rolling_training_early_stopping(ticker, start_date, end_date,\n",
    "                                    n_steps=5, window_size=200, step_size=10,\n",
    "                                    initial_epochs=100, update_epochs=100,\n",
    "                                    initial_lr=0.0005, update_lr=0.0002,\n",
    "                                    n_units=16, batch_size=4,\n",
    "                                    prediction_horizon=126):\n",
    "\n",
    "    X_full, y_full, dates_full = load_full_data(ticker, start_date, end_date, window_size, prediction_horizon)\n",
    "    if X_full is None:\n",
    "        return None, None\n",
    "\n",
    "    start_idx = 0\n",
    "    window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "    if window_data[0] is None:\n",
    "        return None, None\n",
    "    X_samples, y_samples, sample_dates, feat_scaler, target_scaler, (Xw, yw, dw) = window_data\n",
    "\n",
    "    model, val_mape = train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                                      initial_training=True, prev_weights=None,\n",
    "                                      initial_epochs=initial_epochs, update_epochs=update_epochs,\n",
    "                                      initial_lr=initial_lr, update_lr=update_lr,\n",
    "                                      n_units=n_units, batch_size=batch_size)\n",
    "\n",
    "    if model is None:\n",
    "        return None, None\n",
    "\n",
    "    weights_path = f\"rolling_error_analysis_results/{ticker}_initial_6months.weights.h5\"\n",
    "    model.save_weights(weights_path)\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    pred_dates = []\n",
    "\n",
    "    iteration = 1\n",
    "    while True:\n",
    "        start_idx += step_size\n",
    "        window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "        if window_data[0] is None:\n",
    "            break\n",
    "        X_samples, y_samples, sample_dates, feat_scaler, target_scaler, (Xw, yw, dw) = window_data\n",
    "\n",
    "        model, val_mape = train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                                          initial_training=False, prev_weights=weights_path,\n",
    "                                          initial_epochs=initial_epochs, update_epochs=update_epochs,\n",
    "                                          initial_lr=initial_lr, update_lr=update_lr,\n",
    "                                          n_units=n_units, batch_size=batch_size)\n",
    "\n",
    "        if model is None:\n",
    "            break\n",
    "\n",
    "        weights_path = f\"rolling_error_analysis_results/{ticker}_iteration_{iteration}_6months.weights.h5\"\n",
    "        model.save_weights(weights_path)\n",
    "\n",
    "        X_test_last = X_samples[-1:]\n",
    "        y_test_last = y_samples[-1:]\n",
    "        y_pred_scaled = model.predict(X_test_last, verbose=0)\n",
    "        y_test_unscaled = target_scaler.inverse_transform(y_test_last.reshape(-1,1)).flatten()\n",
    "        y_pred_unscaled = target_scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "        predictions.append(y_pred_unscaled[0])\n",
    "        actuals.append(y_test_unscaled[0])\n",
    "        pred_dates.append(sample_dates[-1])\n",
    "        iteration += 1\n",
    "\n",
    "    if len(predictions) == 0:\n",
    "        return None, None\n",
    "\n",
    "    df_results = pd.DataFrame({\n",
    "        'Date': pred_dates,\n",
    "        'Actual': actuals,\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "    df_results['Absolute_Error'] = np.abs(df_results['Actual'] - df_results['Predicted'])\n",
    "    df_results['APE'] = df_results['Absolute_Error'] / np.abs(df_results['Actual'])\n",
    "    df_results['MAPE'] = df_results['APE'] * 100.0\n",
    "    overall_mape = df_results['MAPE'].mean()\n",
    "\n",
    "    return overall_mape, df_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mape, results_df = rolling_training_early_stopping(\n",
    "        ticker,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        n_steps=n_steps,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size,\n",
    "        initial_epochs=initial_epochs,\n",
    "        update_epochs=update_epochs,\n",
    "        initial_lr=initial_lr,\n",
    "        update_lr=update_lr,\n",
    "        n_units=16,\n",
    "        batch_size=4,\n",
    "        prediction_horizon=126\n",
    "    )\n",
    "\n",
    "    if mape is not None:\n",
    "        print(f\"[INFO] 6-month horizon rolling training for {ticker} completed. Overall MAPE: {mape:.2f}%\")\n",
    "        results_df.to_csv(\"rolling_error_analysis_results/cost_6months_results1.csv\", index=False)\n",
    "        print(\"[INFO] Detailed results saved to cost_6months_results1.csv\")\n",
    "    else:\n",
    "        print(\"[WARN] Not enough data or no predictions made for COST with 6-month horizon.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b03632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[INFO] COST: SMA MAPE: 12.39%, EMA MAPE: 12.39%, LR MAPE: 12.47%\n",
      "[INFO] Detailed results saved to rolling_error_analysis_results/costco_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import math\n",
    "\n",
    "########################\n",
    "# Configuration for 6-month horizon\n",
    "########################\n",
    "ticker = 'COST'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "window_size = 10\n",
    "step_size = 5\n",
    "prediction_horizon = 126  # Predicting 6 months (126 trading days) ahead\n",
    "history_length = 90  # 90 days of historical data\n",
    "\n",
    "if not os.path.exists(\"rolling_error_analysis_results\"):\n",
    "    os.makedirs(\"rolling_error_analysis_results\")\n",
    "\n",
    "def load_full_data(ticker, start_date, end_date, history_length, prediction_horizon):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    if stock_data.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    stock_data['Close_future'] = stock_data['Close'].shift(-prediction_horizon)\n",
    "    stock_data.dropna(subset=['Close_future'], inplace=True)\n",
    "\n",
    "    # Use the last 90 days of close prices as features\n",
    "    X = []\n",
    "    y = stock_data['Close_future'].values\n",
    "    dates = stock_data['Date'].values\n",
    "\n",
    "    for i in range(history_length, len(stock_data)):\n",
    "        X.append(stock_data['Close'].iloc[i-history_length:i].values)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = y[history_length:]\n",
    "    dates = dates[history_length:]\n",
    "\n",
    "    return X, y, dates\n",
    "\n",
    "def rolling_predictions(ticker, start_date, end_date, history_length, window_size, step_size, prediction_horizon):\n",
    "    X_full, y_full, dates_full = load_full_data(ticker, start_date, end_date, history_length, prediction_horizon)\n",
    "    if X_full is None:\n",
    "        print(f\"[WARN] Not enough data for {ticker}.\")\n",
    "        return None\n",
    "\n",
    "    start_idx = 0\n",
    "    results = []\n",
    "\n",
    "    while start_idx + window_size <= len(X_full):\n",
    "        X_window = X_full[start_idx:start_idx + window_size]\n",
    "        y_window = y_full[start_idx:start_idx + window_size]\n",
    "        dates_window = dates_full[start_idx:start_idx + window_size]\n",
    "\n",
    "        for i in range(len(X_window)):\n",
    "            X_sample = X_window[i]\n",
    "            y_actual = y_window[i]\n",
    "            date = dates_window[i]\n",
    "\n",
    "            # SMA Prediction\n",
    "            sma_pred = np.mean(X_sample[-2:])\n",
    "\n",
    "            # EMA Prediction\n",
    "            alpha = 2 / (2 + 1)\n",
    "            ema_pred = alpha * X_sample[-1] + (1 - alpha) * X_sample[-2]\n",
    "\n",
    "            # Linear Regression Prediction\n",
    "            X_train = np.array([[-2], [-1]])  # Days n-2 and n-1\n",
    "            y_train = X_sample[-2:]\n",
    "            lr_model = LinearRegression()\n",
    "            lr_model.fit(X_train, y_train)\n",
    "            lr_pred = lr_model.predict([[0]])[0]  # Predict day n\n",
    "\n",
    "            results.append({\n",
    "                'Date': date,\n",
    "                'Actual': y_actual,\n",
    "                'SMA_Predicted': sma_pred,\n",
    "                'EMA_Predicted': ema_pred,\n",
    "                'LR_Predicted': lr_pred\n",
    "            })\n",
    "\n",
    "        start_idx += step_size\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['SMA_Absolute_Error'] = np.abs(df_results['Actual'] - df_results['SMA_Predicted'])\n",
    "    df_results['EMA_Absolute_Error'] = np.abs(df_results['Actual'] - df_results['EMA_Predicted'])\n",
    "    df_results['LR_Absolute_Error'] = np.abs(df_results['Actual'] - df_results['LR_Predicted'])\n",
    "    df_results['SMA_MAPE'] = (df_results['SMA_Absolute_Error'] / np.abs(df_results['Actual'])) * 100\n",
    "    df_results['EMA_MAPE'] = (df_results['EMA_Absolute_Error'] / np.abs(df_results['Actual'])) * 100\n",
    "    df_results['LR_MAPE'] = (df_results['LR_Absolute_Error'] / np.abs(df_results['Actual'])) * 100\n",
    "\n",
    "    overall_sma_mape = df_results['SMA_MAPE'].mean()\n",
    "    overall_ema_mape = df_results['EMA_MAPE'].mean()\n",
    "    overall_lr_mape = df_results['LR_MAPE'].mean()\n",
    "\n",
    "    print(f\"[INFO] {ticker}: SMA MAPE: {overall_sma_mape:.2f}%, EMA MAPE: {overall_ema_mape:.2f}%, LR MAPE: {overall_lr_mape:.2f}%\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_df = rolling_predictions(ticker, start_date, end_date, history_length, window_size, step_size, prediction_horizon)\n",
    "\n",
    "    if results_df is not None:\n",
    "        results_path = \"rolling_error_analysis_results/costco_results.csv\"\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"[INFO] Detailed results saved to {results_path}\")\n",
    "    else:\n",
    "        print(\"[WARN] No predictions were made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a2f166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[INFO] No more windows can be extracted for COST.\n",
      "[INFO] 6-month horizon SMA baseline for COST: Overall MAPE: 12.33%\n",
      "[INFO] COST: Total elapsed time: 0.10s\n",
      "[INFO] 6-month horizon SMA baseline completed for COST. MAPE=12.33%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "########################\n",
    "# Configuration for 6-month horizon\n",
    "########################\n",
    "ticker = 'COST'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "n_steps = 5\n",
    "window_size = 200\n",
    "step_size = 10\n",
    "prediction_horizon = 126  # approx 6 months\n",
    "\n",
    "if not os.path.exists(\"baseline_sma_6months_results\"):\n",
    "    os.makedirs(\"baseline_sma_6months_results\")\n",
    "\n",
    "def load_data(ticker, start_date, end_date, window_size, prediction_horizon):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    if stock_data.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    # Shift by prediction_horizon days\n",
    "    stock_data['Close_future'] = stock_data['Close'].shift(-prediction_horizon)\n",
    "    stock_data.dropna(subset=['Close_future'], inplace=True)\n",
    "\n",
    "    X = stock_data['Close'].values\n",
    "    y = stock_data['Close_future'].values\n",
    "    dates = stock_data['Date'].values\n",
    "\n",
    "    if len(dates) < window_size:\n",
    "        return None, None, None\n",
    "\n",
    "    return X, y, dates\n",
    "\n",
    "def extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps):\n",
    "    end_idx = start_idx + window_size\n",
    "    if end_idx > len(X_full):\n",
    "        return None, None, None, None\n",
    "\n",
    "    X_window = X_full[start_idx:end_idx]\n",
    "    y_window = y_full[start_idx:end_idx]\n",
    "    dates_window = dates_full[start_idx:end_idx]\n",
    "\n",
    "    def create_samples(dataX, dataY, n_steps):\n",
    "        X, y, dates = [], [], []\n",
    "        for i in range(len(dataX)-n_steps):\n",
    "            X.append(dataX[i:i+n_steps])\n",
    "            y.append(dataY[i+n_steps-1])\n",
    "            dates.append(dates_window[i+n_steps-1])\n",
    "        return np.array(X), np.array(y), np.array(dates)\n",
    "\n",
    "    X_samples, y_samples, sample_dates = create_samples(X_window, y_window, n_steps)\n",
    "    if X_samples.size == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    return X_samples, y_samples, sample_dates, (X_window, y_window, dates_window)\n",
    "\n",
    "def baseline_sma_prediction(X_samples, n_steps):\n",
    "    predictions = []\n",
    "    for i in range(len(X_samples)):\n",
    "        pred = np.mean(X_samples[i])\n",
    "        predictions.append(pred)\n",
    "    return np.array(predictions)\n",
    "\n",
    "def rolling_baseline_sma_6months(ticker, window_size, step_size, n_steps, prediction_horizon):\n",
    "    X_full, y_full, dates_full = load_data(ticker, start_date, end_date, window_size, prediction_horizon)\n",
    "    if X_full is None:\n",
    "        print(f\"[WARN] Not enough data for {ticker}.\")\n",
    "        return None\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_idx = 0\n",
    "\n",
    "    window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "    if window_data[0] is None:\n",
    "        print(f\"[WARN] Initial window not valid for {ticker}.\")\n",
    "        return None\n",
    "    X_samples, y_samples, sample_dates, _ = window_data\n",
    "\n",
    "    predictions = baseline_sma_prediction(X_samples, n_steps)\n",
    "    all_predictions = list(predictions)\n",
    "    all_actuals = list(y_samples)\n",
    "    all_dates = list(sample_dates)\n",
    "\n",
    "    iteration = 1\n",
    "    while True:\n",
    "        start_idx += step_size\n",
    "        window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "        if window_data[0] is None:\n",
    "            print(f\"[INFO] No more windows can be extracted for {ticker}.\")\n",
    "            break\n",
    "        X_samples, y_samples, sample_dates, _ = window_data\n",
    "        predictions = baseline_sma_prediction(X_samples, n_steps)\n",
    "        all_predictions.extend(predictions)\n",
    "        all_actuals.extend(y_samples)\n",
    "        all_dates.extend(sample_dates)\n",
    "        iteration += 1\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_actuals = np.array(all_actuals)\n",
    "    mape = mean_absolute_percentage_error(all_actuals, all_predictions)*100.0\n",
    "\n",
    "    df_results = pd.DataFrame({\n",
    "        'Date': all_dates,\n",
    "        'Actual': all_actuals,\n",
    "        'Predicted': all_predictions\n",
    "    })\n",
    "    df_results['Absolute_Error'] = np.abs(df_results['Actual'] - df_results['Predicted'])\n",
    "    df_results['APE'] = df_results['Absolute_Error'] / np.abs(df_results['Actual'])\n",
    "    df_results['MAPE'] = df_results['APE'] * 100.0\n",
    "\n",
    "    df_results.to_csv(f\"baseline_sma_6months_results/{ticker}_6months_sma_baseline_results.csv\", index=False)\n",
    "    print(f\"[INFO] 6-month horizon SMA baseline for {ticker}: Overall MAPE: {mape:.2f}%\")\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[INFO] {ticker}: Total elapsed time: {elapsed:.2f}s\")\n",
    "\n",
    "    return mape\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mape = rolling_baseline_sma_6months(ticker, window_size, step_size, n_steps, prediction_horizon)\n",
    "    if mape is not None:\n",
    "        print(f\"[INFO] 6-month horizon SMA baseline completed for {ticker}. MAPE={mape:.2f}%\")\n",
    "    else:\n",
    "        print(\"[WARN] Could not complete SMA baseline for 6-month horizon.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973f897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import talib\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "########################\n",
    "# Configuration for 6-month horizon\n",
    "########################\n",
    "\n",
    "stocks = [\n",
    "    'COST', 'UL', 'AMGN', 'UNH', 'WAT', 'UPS', 'LPX', 'WM', 'NVDA',\n",
    "    'GOOGL', 'MSFT', 'AXP', 'BLK', 'BRK-B', 'NEE', 'XOM', 'CNI'\n",
    "]\n",
    "\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "# Hyperparameters (easily editable)\n",
    "n_steps = 5\n",
    "window_size = 200\n",
    "step_size = 10\n",
    "initial_epochs = 100\n",
    "update_epochs = 100\n",
    "initial_lr = 0.0005\n",
    "update_lr = 0.0002\n",
    "train_ratio = 0.9\n",
    "n_units = 16\n",
    "batch_size = 4\n",
    "prediction_horizon = 126  # 6-month horizon\n",
    "\n",
    "if not os.path.exists(\"rolling_error_analysis_results\"):\n",
    "    os.makedirs(\"rolling_error_analysis_results\")\n",
    "\n",
    "def load_full_data_helper(ticker, start_date, end_date, window_size, prediction_horizon):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    if stock_data.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    stock_data['Close_future'] = stock_data['Close'].shift(-prediction_horizon)\n",
    "    stock_data.dropna(subset=['Close_future'], inplace=True)\n",
    "\n",
    "    upper_band, middle_band, lower_band = talib.BBANDS(stock_data['Close'], timeperiod=90, nbdevup=2, nbdevdn=2)\n",
    "    stock_data['BB_Upper'] = upper_band\n",
    "    stock_data['BB_Lower'] = lower_band\n",
    "    stock_data['Returns'] = stock_data['Close'].pct_change()\n",
    "    stock_data['Volatility_30'] = stock_data['Returns'].rolling(90).std()\n",
    "    stock_data.dropna(subset=['BB_Upper', 'BB_Lower', 'Volatility_30', 'Close_future'], inplace=True)\n",
    "\n",
    "    if len(stock_data) < window_size:\n",
    "        return None, None, None\n",
    "\n",
    "    X_feat = stock_data[['BB_Upper', 'BB_Lower', 'Volatility_30']].values\n",
    "    y_raw = stock_data['Close_future'].values\n",
    "    dates = stock_data['Date'].values\n",
    "    return X_feat, y_raw, dates\n",
    "\n",
    "def load_full_data(ticker, start_date, end_date, window_size, prediction_horizon):\n",
    "    X_feat, y_raw, dates = load_full_data_helper(ticker, start_date, end_date, window_size, prediction_horizon)\n",
    "    if X_feat is None:\n",
    "        return None, None, None\n",
    "    return X_feat, y_raw, dates\n",
    "\n",
    "def extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps):\n",
    "    end_idx = start_idx + window_size\n",
    "    if end_idx > len(X_full):\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    X_window = X_full[start_idx:end_idx]\n",
    "    y_window = y_full[start_idx:end_idx]\n",
    "    dates_window = dates_full[start_idx:end_idx]\n",
    "\n",
    "    def lstm_split(dataX, dataY, n_steps):\n",
    "        X, y = [], []\n",
    "        for i in range(len(dataX)-n_steps+1):\n",
    "            X.append(dataX[i:i+n_steps])\n",
    "            y.append(dataY[i+n_steps-1])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X_samples, y_samples = lstm_split(X_window, y_window, n_steps)\n",
    "    sample_dates = dates_window[n_steps-1:]\n",
    "\n",
    "    feat_scaler = RobustScaler()\n",
    "    target_scaler = RobustScaler()\n",
    "    X_flat = X_window.reshape(len(X_window), -1)\n",
    "    feat_scaler.fit(X_flat)\n",
    "    y_window_reshaped = y_window.reshape(-1,1)\n",
    "    target_scaler.fit(y_window_reshaped)\n",
    "    X_scaled = feat_scaler.transform(X_flat).reshape(X_window.shape)\n",
    "    y_scaled = target_scaler.transform(y_window_reshaped).flatten()\n",
    "    X_samples_scaled, y_samples_scaled = lstm_split(X_scaled, y_scaled, n_steps)\n",
    "\n",
    "    return X_samples_scaled, y_samples_scaled, sample_dates, feat_scaler, target_scaler, (X_window, y_window, dates_window)\n",
    "\n",
    "def create_lstm_model(n_units, learning_rate, n_steps, n_features, n_layers=1):\n",
    "    inputs = Input(shape=(n_steps, n_features))\n",
    "    x = LSTM(n_units, activation='relu', return_sequences=(n_layers > 1))(inputs)\n",
    "    if n_layers > 1:\n",
    "        x = LSTM(n_units, activation='relu')(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                    initial_training=False, prev_weights=None,\n",
    "                    initial_epochs=100, update_epochs=100,\n",
    "                    initial_lr=0.0005, update_lr=0.0002,\n",
    "                    n_units=16, batch_size=4, train_ratio=0.9):\n",
    "    total_samples = len(X_samples)\n",
    "    train_count = int(math.floor(total_samples * train_ratio))\n",
    "    if train_count >= total_samples:\n",
    "        train_count = total_samples - 1\n",
    "\n",
    "    X_train, y_train = X_samples[:train_count], y_samples[:train_count]\n",
    "    X_val, y_val = X_samples[train_count:], y_samples[train_count:]\n",
    "\n",
    "    if X_train.size == 0 or X_val.size == 0:\n",
    "        return None, np.nan\n",
    "\n",
    "    n_steps_local = X_train.shape[1]\n",
    "    n_features = X_train.shape[2]\n",
    "\n",
    "    if initial_training:\n",
    "        epochs = initial_epochs\n",
    "        lr = initial_lr\n",
    "    else:\n",
    "        epochs = update_epochs\n",
    "        lr = update_lr\n",
    "\n",
    "    model = create_lstm_model(n_units, lr, n_steps_local, n_features, n_layers=1)\n",
    "\n",
    "    if prev_weights is not None:\n",
    "        model.load_weights(prev_weights)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=lr))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False,\n",
    "              validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "    y_val_pred_scaled = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = target_scaler.inverse_transform(y_val.reshape(-1,1)).flatten()\n",
    "    y_val_pred_unscaled = target_scaler.inverse_transform(y_val_pred_scaled).flatten()\n",
    "    val_mape = mean_absolute_percentage_error(y_val_unscaled, y_val_pred_unscaled) * 100\n",
    "\n",
    "    return model, val_mape\n",
    "\n",
    "def rolling_training_early_stopping(ticker, start_date, end_date,\n",
    "                                    n_steps=5, window_size=200, step_size=10,\n",
    "                                    initial_epochs=100, update_epochs=100,\n",
    "                                    initial_lr=0.0005, update_lr=0.0002,\n",
    "                                    n_units=16, batch_size=4,\n",
    "                                    prediction_horizon=126, train_ratio=0.9):\n",
    "    X_full, y_full, dates_full = load_full_data(ticker, start_date, end_date, window_size, prediction_horizon)\n",
    "    if X_full is None:\n",
    "        return None\n",
    "\n",
    "    start_idx = 0\n",
    "    window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "    if window_data[0] is None:\n",
    "        return None\n",
    "    X_samples, y_samples, sample_dates, feat_scaler, target_scaler, (Xw, yw, dw) = window_data\n",
    "\n",
    "    model, val_mape = train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                                      initial_training=True, prev_weights=None,\n",
    "                                      initial_epochs=initial_epochs, update_epochs=update_epochs,\n",
    "                                      initial_lr=initial_lr, update_lr=update_lr,\n",
    "                                      n_units=n_units, batch_size=batch_size, train_ratio=train_ratio)\n",
    "\n",
    "    if model is None:\n",
    "        return None\n",
    "\n",
    "    weights_path = f\"rolling_error_analysis_results/{ticker}_initial_6months.weights.h5\"\n",
    "    model.save_weights(weights_path)\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    pred_dates = []\n",
    "\n",
    "    iteration = 1\n",
    "    while True:\n",
    "        start_idx += step_size\n",
    "        window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "        if window_data[0] is None:\n",
    "            break\n",
    "        X_samples, y_samples, sample_dates, feat_scaler, target_scaler, (Xw, yw, dw) = window_data\n",
    "\n",
    "        model, val_mape = train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                                          initial_training=False, prev_weights=weights_path,\n",
    "                                          initial_epochs=initial_epochs, update_epochs=update_epochs,\n",
    "                                          initial_lr=initial_lr, update_lr=update_lr,\n",
    "                                          n_units=n_units, batch_size=batch_size, train_ratio=train_ratio)\n",
    "\n",
    "        if model is None:\n",
    "            break\n",
    "\n",
    "        weights_path = f\"rolling_error_analysis_results/{ticker}_iteration_{iteration}_6months.weights.h5\"\n",
    "        model.save_weights(weights_path)\n",
    "\n",
    "        X_test_last = X_samples[-1:]\n",
    "        y_test_last = y_samples[-1:]\n",
    "        y_pred_scaled = model.predict(X_test_last, verbose=0)\n",
    "        y_test_unscaled = target_scaler.inverse_transform(y_test_last.reshape(-1,1)).flatten()\n",
    "        y_pred_unscaled = target_scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "        predictions.append(y_pred_unscaled[0])\n",
    "        actuals.append(y_test_unscaled[0])\n",
    "        pred_dates.append(sample_dates[-1])\n",
    "        iteration += 1\n",
    "\n",
    "    if len(predictions) == 0:\n",
    "        return None\n",
    "\n",
    "    df_results = pd.DataFrame({\n",
    "        'Date': pred_dates,\n",
    "        'Actual': actuals,\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "    df_results['Absolute_Error'] = np.abs(df_results['Actual'] - df_results['Predicted'])\n",
    "    df_results['APE'] = df_results['Absolute_Error'] / np.abs(df_results['Actual'])\n",
    "    df_results['MAPE'] = df_results['APE'] * 100.0\n",
    "    overall_mape = df_results['MAPE'].mean()\n",
    "\n",
    "    return overall_mape\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Multi-ticker run: run for each stock and save MAPEs\n",
    "    results = []\n",
    "    for s in stocks:\n",
    "        mape = rolling_training_early_stopping(\n",
    "            s,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            n_steps=n_steps,\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            initial_epochs=initial_epochs,\n",
    "            update_epochs=update_epochs,\n",
    "            initial_lr=initial_lr,\n",
    "            update_lr=update_lr,\n",
    "            n_units=n_units,\n",
    "            batch_size=batch_size,\n",
    "            prediction_horizon=prediction_horizon,\n",
    "            train_ratio=train_ratio\n",
    "        )\n",
    "        if mape is not None:\n",
    "            results.append({'Stock': s, 'Overall_MAPE': mape})\n",
    "        else:\n",
    "            results.append({'Stock': s, 'Overall_MAPE': np.nan})\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('stock_mapes_6months.csv', index=False)\n",
    "    print(\"[INFO] All stock MAPEs saved to stock_mapes_6months.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "########################\n",
    "# Configuration for 6-month horizon\n",
    "########################\n",
    "\n",
    "stocks = [\n",
    "    'COST', 'UL', 'AMGN', 'UNH', 'WAT', 'UPS', 'LPX', 'WM', 'NVDA',\n",
    "    'GOOGL', 'MSFT', 'AXP', 'BLK', 'BRK-B', 'NEE', 'XOM', 'CNI'\n",
    "]\n",
    "\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "n_steps = 5\n",
    "window_size = 200\n",
    "step_size = 10\n",
    "prediction_horizon = 126  # approx 6 months\n",
    "\n",
    "if not os.path.exists(\"baseline_sma_6months_results\"):\n",
    "    os.makedirs(\"baseline_sma_6months_results\")\n",
    "\n",
    "def load_data(ticker, start_date, end_date, window_size, prediction_horizon):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    if stock_data.empty:\n",
    "        return None, None, None\n",
    "    stock_data['Close_future'] = stock_data['Close'].shift(-prediction_horizon)\n",
    "    stock_data.dropna(subset=['Close_future'], inplace=True)\n",
    "\n",
    "    X = stock_data['Close'].values\n",
    "    y = stock_data['Close_future'].values\n",
    "    dates = stock_data['Date'].values\n",
    "\n",
    "    if len(dates) < window_size:\n",
    "        return None, None, None\n",
    "\n",
    "    return X, y, dates\n",
    "\n",
    "def extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps):\n",
    "    end_idx = start_idx + window_size\n",
    "    if end_idx > len(X_full):\n",
    "        return None, None, None, None\n",
    "\n",
    "    X_window = X_full[start_idx:end_idx]\n",
    "    y_window = y_full[start_idx:end_idx]\n",
    "    dates_window = dates_full[start_idx:end_idx]\n",
    "\n",
    "    def create_samples(dataX, dataY, n_steps):\n",
    "        X, y, dates = [], [], []\n",
    "        for i in range(len(dataX)-n_steps):\n",
    "            X.append(dataX[i:i+n_steps])\n",
    "            y.append(dataY[i+n_steps-1])\n",
    "            dates.append(dates_window[i+n_steps-1])\n",
    "        return np.array(X), np.array(y), np.array(dates)\n",
    "\n",
    "    X_samples, y_samples, sample_dates = create_samples(X_window, y_window, n_steps)\n",
    "    if X_samples.size == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    return X_samples, y_samples, sample_dates, (X_window, y_window, dates_window)\n",
    "\n",
    "def baseline_sma_prediction(X_samples, n_steps):\n",
    "    predictions = []\n",
    "    for i in range(len(X_samples)):\n",
    "        pred = np.mean(X_samples[i])\n",
    "        predictions.append(pred)\n",
    "    return np.array(predictions)\n",
    "\n",
    "def rolling_baseline_sma_6months(ticker, window_size, step_size, n_steps, prediction_horizon):\n",
    "    X_full, y_full, dates_full = load_data(ticker, start_date, end_date, window_size, prediction_horizon)\n",
    "    if X_full is None:\n",
    "        print(f\"[WARN] Not enough data for {ticker}.\")\n",
    "        return None\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_idx = 0\n",
    "\n",
    "    window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "    if window_data[0] is None:\n",
    "        print(f\"[WARN] Initial window not valid for {ticker}.\")\n",
    "        return None\n",
    "    X_samples, y_samples, sample_dates, _ = window_data\n",
    "\n",
    "    predictions = baseline_sma_prediction(X_samples, n_steps)\n",
    "    all_predictions = list(predictions)\n",
    "    all_actuals = list(y_samples)\n",
    "    all_dates = list(sample_dates)\n",
    "\n",
    "    iteration = 1\n",
    "    while True:\n",
    "        start_idx += step_size\n",
    "        window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "        if window_data[0] is None:\n",
    "            print(f\"[INFO] No more windows can be extracted for {ticker}.\")\n",
    "            break\n",
    "        X_samples, y_samples, sample_dates, _ = window_data\n",
    "        predictions = baseline_sma_prediction(X_samples, n_steps)\n",
    "        all_predictions.extend(predictions)\n",
    "        all_actuals.extend(y_samples)\n",
    "        all_dates.extend(sample_dates)\n",
    "        iteration += 1\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_actuals = np.array(all_actuals)\n",
    "    mape = mean_absolute_percentage_error(all_actuals, all_predictions)*100.0\n",
    "\n",
    "    df_results = pd.DataFrame({\n",
    "        'Date': all_dates,\n",
    "        'Actual': all_actuals,\n",
    "        'Predicted': all_predictions\n",
    "    })\n",
    "    df_results['Absolute_Error'] = np.abs(df_results['Actual'] - df_results['Predicted'])\n",
    "    df_results['APE'] = df_results['Absolute_Error'] / np.abs(df_results['Actual'])\n",
    "    df_results['MAPE'] = df_results['APE'] * 100.0\n",
    "\n",
    "    df_results.to_csv(f\"baseline_sma_6months_results/{ticker}_6months_sma_baseline_results.csv\", index=False)\n",
    "    print(f\"[INFO] 6-month horizon SMA baseline for {ticker}: Overall MAPE: {mape:.2f}%\")\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[INFO] {ticker}: Total elapsed time: {elapsed:.2f}s\")\n",
    "\n",
    "    return mape\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = []\n",
    "    for s in stocks:\n",
    "        mape = rolling_baseline_sma_6months(s, window_size, step_size, n_steps, prediction_horizon)\n",
    "        if mape is not None:\n",
    "            results.append({'Stock': s, 'Overall_MAPE': mape})\n",
    "        else:\n",
    "            results.append({'Stock': s, 'Overall_MAPE': np.nan})\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('stock_mapes_6months_sma_baseline.csv', index=False)\n",
    "    print(\"[INFO] All SMA baseline MAPEs for 6-month horizon saved to stock_mapes_6months_sma_baseline.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import talib\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "########################\n",
    "# Configuration for 6-month horizon\n",
    "########################\n",
    "\n",
    "ticker = 'COST'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "# Hyperparameters (easily editable)\n",
    "n_steps = 5\n",
    "window_size = 200\n",
    "step_size = 10\n",
    "initial_epochs = 100\n",
    "update_epochs = 100\n",
    "initial_lr = 0.0005\n",
    "update_lr = 0.0002\n",
    "train_ratio = 0.9\n",
    "n_units = 16\n",
    "batch_size = 4\n",
    "prediction_horizon = 126  # 6-month horizon\n",
    "\n",
    "if not os.path.exists(\"rolling_error_analysis_results\"):\n",
    "    os.makedirs(\"rolling_error_analysis_results\")\n",
    "\n",
    "def load_full_data_helper(ticker, start_date, end_date, window_size, prediction_horizon):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    if stock_data.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    stock_data['Close_future'] = stock_data['Close'].shift(-prediction_horizon)\n",
    "    stock_data.dropna(subset=['Close_future'], inplace=True)\n",
    "    upper_band, middle_band, lower_band = talib.BBANDS(stock_data['Close'], timeperiod=90, nbdevup=2, nbdevdn=2)\n",
    "    stock_data['BB_Upper'] = upper_band\n",
    "    stock_data['BB_Lower'] = lower_band\n",
    "    stock_data['Returns'] = stock_data['Close'].pct_change()\n",
    "    stock_data['Volatility_30'] = stock_data['Returns'].rolling(90).std()\n",
    "    stock_data.dropna(subset=['BB_Upper', 'BB_Lower', 'Volatility_30', 'Close_future'], inplace=True)\n",
    "\n",
    "    if len(stock_data) < window_size:\n",
    "        return None, None, None\n",
    "\n",
    "    X_feat = stock_data[['BB_Upper', 'BB_Lower', 'Volatility_30']].values\n",
    "    y_raw = stock_data['Close_future'].values\n",
    "    dates = stock_data['Date'].values\n",
    "    return X_feat, y_raw, dates\n",
    "\n",
    "def load_full_data(ticker, start_date, end_date, window_size, prediction_horizon):\n",
    "    X_feat, y_raw, dates = load_full_data_helper(ticker, start_date, end_date, window_size, prediction_horizon)\n",
    "    if X_feat is None:\n",
    "        return None, None, None\n",
    "    return X_feat, y_raw, dates\n",
    "\n",
    "def extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps):\n",
    "    end_idx = start_idx + window_size\n",
    "    if end_idx > len(X_full):\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    X_window = X_full[start_idx:end_idx]\n",
    "    y_window = y_full[start_idx:end_idx]\n",
    "    dates_window = dates_full[start_idx:end_idx]\n",
    "\n",
    "    def lstm_split(dataX, dataY, n_steps):\n",
    "        X, y = [], []\n",
    "        for i in range(len(dataX)-n_steps+1):\n",
    "            X.append(dataX[i:i+n_steps])\n",
    "            y.append(dataY[i+n_steps-1])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X_samples, y_samples = lstm_split(X_window, y_window, n_steps)\n",
    "    sample_dates = dates_window[n_steps-1:]\n",
    "\n",
    "    feat_scaler = RobustScaler()\n",
    "    target_scaler = RobustScaler()\n",
    "    X_flat = X_window.reshape(len(X_window), -1)\n",
    "    feat_scaler.fit(X_flat)\n",
    "    y_window_reshaped = y_window.reshape(-1,1)\n",
    "    target_scaler.fit(y_window_reshaped)\n",
    "    X_scaled = feat_scaler.transform(X_flat).reshape(X_window.shape)\n",
    "    y_scaled = target_scaler.transform(y_window_reshaped).flatten()\n",
    "    X_samples_scaled, y_samples_scaled = lstm_split(X_scaled, y_scaled, n_steps)\n",
    "\n",
    "    return X_samples_scaled, y_samples_scaled, sample_dates, feat_scaler, target_scaler, (X_window, y_window, dates_window)\n",
    "\n",
    "def create_lstm_model(n_units, learning_rate, n_steps, n_features, n_layers=1):\n",
    "    inputs = Input(shape=(n_steps, n_features))\n",
    "    x = LSTM(n_units, activation='relu', return_sequences=(n_layers > 1))(inputs)\n",
    "    if n_layers > 1:\n",
    "        x = LSTM(n_units, activation='relu')(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                    initial_training=False, prev_weights=None,\n",
    "                    initial_epochs=100, update_epochs=100,\n",
    "                    initial_lr=0.0005, update_lr=0.0002,\n",
    "                    n_units=16, batch_size=4):\n",
    "    total_samples = len(X_samples)\n",
    "    global train_ratio\n",
    "    train_count = int(math.floor(total_samples * train_ratio))\n",
    "    if train_count >= total_samples:\n",
    "        train_count = total_samples - 1\n",
    "\n",
    "    X_train, y_train = X_samples[:train_count], y_samples[:train_count]\n",
    "    X_val, y_val = X_samples[train_count:], y_samples[train_count:]\n",
    "\n",
    "    if X_train.size == 0 or X_val.size == 0:\n",
    "        return None, np.nan\n",
    "\n",
    "    n_steps_local = X_train.shape[1]\n",
    "    n_features = X_train.shape[2]\n",
    "\n",
    "    if initial_training:\n",
    "        epochs = initial_epochs\n",
    "        lr = initial_lr\n",
    "    else:\n",
    "        epochs = update_epochs\n",
    "        lr = update_lr\n",
    "\n",
    "    model = create_lstm_model(n_units, lr, n_steps_local, n_features, n_layers=1)\n",
    "\n",
    "    if prev_weights is not None:\n",
    "        model.load_weights(prev_weights)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=lr))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False,\n",
    "              validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "    y_val_pred_scaled = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = target_scaler.inverse_transform(y_val.reshape(-1,1)).flatten()\n",
    "    y_val_pred_unscaled = target_scaler.inverse_transform(y_val_pred_scaled).flatten()\n",
    "    val_mape = mean_absolute_percentage_error(y_val_unscaled, y_val_pred_unscaled) * 100\n",
    "\n",
    "    return model, val_mape\n",
    "\n",
    "def rolling_training_early_stopping(ticker, start_date, end_date,\n",
    "                                    n_steps=5, window_size=200, step_size=10,\n",
    "                                    initial_epochs=100, update_epochs=100,\n",
    "                                    initial_lr=0.0005, update_lr=0.0002,\n",
    "                                    n_units=16, batch_size=4,\n",
    "                                    prediction_horizon=126):\n",
    "    X_full, y_full, dates_full = load_full_data(ticker, start_date, end_date, window_size, prediction_horizon)\n",
    "    if X_full is None:\n",
    "        return None, None\n",
    "\n",
    "    start_idx = 0\n",
    "    window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "    if window_data[0] is None:\n",
    "        return None, None\n",
    "    X_samples, y_samples, sample_dates, feat_scaler, target_scaler, (Xw, yw, dw) = window_data\n",
    "\n",
    "    model, val_mape = train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                                      initial_training=True, prev_weights=None,\n",
    "                                      initial_epochs=initial_epochs, update_epochs=update_epochs,\n",
    "                                      initial_lr=initial_lr, update_lr=update_lr,\n",
    "                                      n_units=n_units, batch_size=batch_size)\n",
    "\n",
    "    if model is None:\n",
    "        return None, None\n",
    "\n",
    "    weights_path = f\"rolling_error_analysis_results/{ticker}_initial_6months.weights.h5\"\n",
    "    model.save_weights(weights_path)\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    pred_dates = []\n",
    "\n",
    "    iteration = 1\n",
    "    while True:\n",
    "        start_idx += step_size\n",
    "        window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "        if window_data[0] is None:\n",
    "            break\n",
    "        X_samples, y_samples, sample_dates, feat_scaler, target_scaler, (Xw, yw, dw) = window_data\n",
    "\n",
    "        model, val_mape = train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                                          initial_training=False, prev_weights=weights_path,\n",
    "                                          initial_epochs=initial_epochs, update_epochs=update_epochs,\n",
    "                                          initial_lr=initial_lr, update_lr=update_lr,\n",
    "                                          n_units=n_units, batch_size=batch_size)\n",
    "\n",
    "        if model is None:\n",
    "            break\n",
    "\n",
    "        weights_path = f\"rolling_error_analysis_results/{ticker}_iteration_{iteration}_6months.weights.h5\"\n",
    "        model.save_weights(weights_path)\n",
    "\n",
    "        X_test_last = X_samples[-1:]\n",
    "        y_test_last = y_samples[-1:]\n",
    "        y_pred_scaled = model.predict(X_test_last, verbose=0)\n",
    "        y_test_unscaled = target_scaler.inverse_transform(y_test_last.reshape(-1,1)).flatten()\n",
    "        y_pred_unscaled = target_scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "        predictions.append(y_pred_unscaled[0])\n",
    "        actuals.append(y_test_unscaled[0])\n",
    "        pred_dates.append(sample_dates[-1])\n",
    "        iteration += 1\n",
    "\n",
    "    if len(predictions) == 0:\n",
    "        return None, None\n",
    "\n",
    "    df_results = pd.DataFrame({\n",
    "        'Date': pred_dates,\n",
    "        'Actual': actuals,\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "    df_results['Absolute_Error'] = np.abs(df_results['Actual'] - df_results['Predicted'])\n",
    "    df_results['APE'] = df_results['Absolute_Error'] / np.abs(df_results['Actual'])\n",
    "    df_results['MAPE'] = df_results['APE'] * 100.0\n",
    "    overall_mape = df_results['MAPE'].mean()\n",
    "\n",
    "    return overall_mape, df_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mape, results_df = rolling_training_early_stopping(\n",
    "        ticker,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        n_steps=n_steps,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size,\n",
    "        initial_epochs=initial_epochs,\n",
    "        update_epochs=update_epochs,\n",
    "        initial_lr=initial_lr,\n",
    "        update_lr=update_lr,\n",
    "        n_units=n_units,\n",
    "        batch_size=batch_size,\n",
    "        prediction_horizon=prediction_horizon\n",
    "    )\n",
    "\n",
    "    if mape is not None:\n",
    "        print(f\"[INFO] 6-month horizon rolling training for {ticker} completed. Overall MAPE: {mape:.2f}%\")\n",
    "        results_df.to_csv(\"rolling_error_analysis_results/cost_6months_results1.csv\", index=False)\n",
    "        print(\"[INFO] Detailed results saved to cost_6months_results1.csv\")\n",
    "    else:\n",
    "        print(\"[WARN] Not enough data or no predictions made for COST with 6-month horizon.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b9d4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
