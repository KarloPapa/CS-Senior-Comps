{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db92326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import talib\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "########################\n",
    "# Configuration\n",
    "########################\n",
    "ticker = 'AXP'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-01-01'\n",
    "1\n",
    "# Hyperparameters\n",
    "n_steps = 2\n",
    "window_size = 5\n",
    "step_size = 2\n",
    "initial_epochs = 100\n",
    "update_epochs = 100\n",
    "initial_lr = 0.0005\n",
    "update_lr = 0.00005\n",
    "train_ratio = 0.9\n",
    "n_units = 16\n",
    "batch_size = 4\n",
    "\n",
    "# If you change features or other logic, do so below\n",
    "if not os.path.exists(\"rolling_error_analysis_results\"):\n",
    "    os.makedirs(\"rolling_error_analysis_results\")\n",
    "\n",
    "\n",
    "def load_full_data_helper(ticker, start_date, end_date, window_size):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    if stock_data.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    # Next day prediction setup\n",
    "    stock_data['Close_next_day'] = stock_data['Close'].shift(-1)\n",
    "    stock_data.dropna(subset=['Close_next_day'], inplace=True)\n",
    "\n",
    "    # Simple features\n",
    "    X_feat = stock_data[['Open', 'High', 'Low']]\n",
    "    y_raw = stock_data['Close_next_day'].values\n",
    "    dates = stock_data['Date'].values\n",
    "\n",
    "    if len(dates) < window_size:\n",
    "        return None, None, None\n",
    "    return X_feat.values, y_raw, dates\n",
    "\n",
    "\n",
    "def load_full_data(ticker, start_date, end_date, window_size):\n",
    "    X_feat, y_raw, dates = load_full_data_helper(ticker, start_date, end_date, window_size)\n",
    "    if X_feat is None:\n",
    "        return None, None, None\n",
    "    return X_feat, y_raw, dates\n",
    "\n",
    "\n",
    "def extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps):\n",
    "    end_idx = start_idx + window_size\n",
    "    if end_idx > len(X_full):\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    X_window = X_full[start_idx:end_idx]\n",
    "    y_window = y_full[start_idx:end_idx]\n",
    "    dates_window = dates_full[start_idx:end_idx]\n",
    "\n",
    "    def lstm_split(dataX, dataY, n_steps):\n",
    "        X, y = [], []\n",
    "        for i in range(len(dataX)-n_steps+1):\n",
    "            X.append(dataX[i:i+n_steps])\n",
    "            y.append(dataY[i+n_steps-1])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X_samples, y_samples = lstm_split(X_window, y_window, n_steps)\n",
    "    sample_dates = dates_window[n_steps-1:]\n",
    "\n",
    "    feat_scaler = RobustScaler()\n",
    "    target_scaler = RobustScaler()\n",
    "    X_flat = X_window.reshape(len(X_window), -1)\n",
    "    feat_scaler.fit(X_flat)\n",
    "    y_window_reshaped = y_window.reshape(-1,1)\n",
    "    target_scaler.fit(y_window_reshaped)\n",
    "\n",
    "    X_scaled = feat_scaler.transform(X_flat).reshape(X_window.shape)\n",
    "    y_scaled = target_scaler.transform(y_window_reshaped).flatten()\n",
    "    X_samples_scaled, y_samples_scaled = lstm_split(X_scaled, y_scaled, n_steps)\n",
    "\n",
    "    return X_samples_scaled, y_samples_scaled, sample_dates, feat_scaler, target_scaler, (X_window, y_window, dates_window)\n",
    "\n",
    "\n",
    "def create_lstm_model(n_units, learning_rate, n_steps, n_features, n_layers=1):\n",
    "    inputs = Input(shape=(n_steps, n_features))\n",
    "    x = LSTM(n_units, activation='relu', return_sequences=(n_layers > 1))(inputs)\n",
    "    if n_layers > 1:\n",
    "        x = LSTM(n_units, activation='relu')(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    # We'll compile after loading weights if needed\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_on_window(X_samples, y_samples, feat_scaler, target_scaler,\n",
    "                    initial_training=False, prev_weights=None):\n",
    "    total_samples = len(X_samples)\n",
    "    global train_ratio\n",
    "    train_count = int(math.floor(total_samples * train_ratio))\n",
    "    if train_count >= total_samples:\n",
    "        train_count = total_samples - 1\n",
    "\n",
    "    X_train, y_train = X_samples[:train_count], y_samples[:train_count]\n",
    "    X_val, y_val = X_samples[train_count:], y_samples[train_count:]\n",
    "\n",
    "    if X_train.size == 0 or X_val.size == 0:\n",
    "        return None, np.nan\n",
    "\n",
    "    n_steps_local = X_train.shape[1]\n",
    "    n_features = X_train.shape[2]\n",
    "\n",
    "    if initial_training:\n",
    "        epochs = initial_epochs\n",
    "        lr = initial_lr\n",
    "    else:\n",
    "        epochs = update_epochs\n",
    "        lr = update_lr\n",
    "\n",
    "    model = create_lstm_model(n_units, lr, n_steps_local, n_features, n_layers=1)\n",
    "\n",
    "    if prev_weights is not None:\n",
    "        model.load_weights(prev_weights)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=lr))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False,\n",
    "              validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "    y_val_pred_scaled = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = target_scaler.inverse_transform(y_val.reshape(-1,1)).flatten()\n",
    "    y_val_pred_unscaled = target_scaler.inverse_transform(y_val_pred_scaled).flatten()\n",
    "    val_mape = mean_absolute_percentage_error(y_val_unscaled, y_val_pred_unscaled) * 100\n",
    "\n",
    "    return model, val_mape\n",
    "\n",
    "\n",
    "def rolling_training_early_stopping(ticker, window_size, step_size, n_steps):\n",
    "    X_full, y_full, dates_full = load_full_data(ticker, start_date, end_date, window_size)\n",
    "    if X_full is None:\n",
    "        print(\"[WARN] Not enough data.\")\n",
    "        return\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    start_idx = 0\n",
    "    window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "    if window_data[0] is None:\n",
    "        print(\"[WARN] Initial window not valid.\")\n",
    "        return\n",
    "    X_samples, y_samples, sample_dates, feat_scaler, target_scaler, (Xw, yw, dw) = window_data\n",
    "\n",
    "    print(\"[INFO] Training initial window with Early Stopping...\")\n",
    "    model, val_mape = train_on_window(X_samples, y_samples, feat_scaler, target_scaler, initial_training=True, prev_weights=None)\n",
    "    print(f\"[RESULT] Initial window validation MAPE: {val_mape:.2f}%\")\n",
    "\n",
    "    weights_path = \"rolling_error_analysis_results/cost_initial_es.weights.h5\"\n",
    "    model.save_weights(weights_path)\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    pred_dates = []\n",
    "\n",
    "    iteration = 1\n",
    "    while True:\n",
    "        start_idx += step_size\n",
    "        window_data = extract_window(X_full, y_full, dates_full, start_idx, window_size, n_steps)\n",
    "        if window_data[0] is None:\n",
    "            print(\"[INFO] No more windows can be extracted.\")\n",
    "            break\n",
    "        X_samples, y_samples, sample_dates, feat_scaler, target_scaler, (Xw, yw, dw) = window_data\n",
    "\n",
    "        print(f\"\\n[INFO] Iteration {iteration}: training on new window with Early Stopping...\")\n",
    "        model, val_mape = train_on_window(X_samples, y_samples, feat_scaler, target_scaler, initial_training=False, prev_weights=weights_path)\n",
    "        print(f\"[RESULT] Window validation MAPE: {val_mape:.2f}%\")\n",
    "\n",
    "        weights_path = f\"rolling_error_analysis_results/cost_iteration_{iteration}_es.weights.h5\"\n",
    "        model.save_weights(weights_path)\n",
    "\n",
    "        X_test_last = X_samples[-1:]\n",
    "        y_test_last = y_samples[-1:]\n",
    "        y_pred_scaled = model.predict(X_test_last, verbose=0)\n",
    "        y_test_unscaled = target_scaler.inverse_transform(y_test_last.reshape(-1,1)).flatten()\n",
    "        y_pred_unscaled = target_scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "        predictions.append(y_pred_unscaled[0])\n",
    "        actuals.append(y_test_unscaled[0])\n",
    "        pred_dates.append(sample_dates[-1])\n",
    "\n",
    "        test_mape = mean_absolute_percentage_error(y_test_unscaled, y_pred_unscaled)*100\n",
    "        print(f\"[INFO] Predicted last sample of window: Actual={y_test_unscaled[0]:.2f}, Pred={y_pred_unscaled[0]:.2f}, MAPE={test_mape:.2f}%\")\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    df_results = pd.DataFrame({\n",
    "        'Date': pred_dates,\n",
    "        'Actual': actuals,\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "    df_results['Absolute_Error'] = np.abs(df_results['Actual'] - df_results['Predicted'])\n",
    "    df_results['APE'] = df_results['Absolute_Error'] / np.abs(df_results['Actual'])\n",
    "    df_results['MAPE'] = df_results['APE'] * 100.0\n",
    "\n",
    "    df_results.to_csv(\"rolling_error_analysis_results/cost_rolling_results_es.csv\", index=False)\n",
    "    overall_mape = df_results['MAPE'].mean()\n",
    "    print(f\"\\n[INFO] Rolling training with Early Stopping completed for {ticker}. Overall MAPE: {overall_mape:.2f}%\")\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[INFO] Total elapsed time: {elapsed:.2f}s\")\n",
    "\n",
    "    worst_10 = df_results.sort_values('MAPE', ascending=False).head(10)\n",
    "    print(\"[INFO] Top 10 worst predictions in rolling scenario (Early Stopping):\")\n",
    "    print(worst_10[['Date', 'Actual', 'Predicted', 'MAPE']])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Just run with the current top-level defined parameters:\n",
    "    rolling_training_early_stopping(ticker, window_size, step_size, n_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273017e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
